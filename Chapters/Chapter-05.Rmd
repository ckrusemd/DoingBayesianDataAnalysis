---
title: "Chapter 5"
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE,scipen=999)
options(scipen=999)
```

# Chapter 5: Bayes' rule

> Bayes' rule is merely the mathematical relation between the prior allocation of credibility and the posterior reallocation of credibility conditional on data.

*There is another branch of statistics, called frequentist, which does not use Bayes' rule for inference and decisions.*

## 5.1.1 Derived from definitions of conditional probability

![](images/5_1.png)

![](images/5_2.png)

In words, the definition simply says that the probability of c given r is the probability that *they happen together* **relative to** *the probability that r happens at all*.

> E.g.: **P of blond hair WHEN having blue eyes** = **P of blue eyes AND blond hair** relative to **P of blue eyes at all**

![](images/5_3.png)

> E.g.: **P of blond hair WHEN having blue eyes** = **P of blue eyes when blond hair * P of blond hair**  relative to **P of blue eyes at all**

```{r}

library(datasets)
data(HairEyeColor)

HairEyeColor.male = t(HairEyeColor[,,"Male"])

HairEyeColor.male.prop = prop.table(HairEyeColor.male)

HairEyeColor.male.prop.matrix = 
  as.data.frame(matrix(HairEyeColor.male.prop,ncol=4,nrow=4))

colnames(HairEyeColor.male.prop.matrix) = c("Black","Brunette","Red","Blond")
rownames(HairEyeColor.male.prop.matrix) = c("Brown","Blue","Hazel","Green")

HairEyeColor.male.prop.matrix$Marginal_Eye = as.numeric(rowSums(HairEyeColor.male.prop.matrix))
HairEyeColor.male.prop.matrix = as.data.frame(t(HairEyeColor.male.prop.matrix))

HairEyeColor.male.prop.matrix$Marginal_Hair= as.numeric(rowSums(HairEyeColor.male.prop.matrix))
HairEyeColor.male.prop.matrix = as.data.frame(t(HairEyeColor.male.prop.matrix))

HairEyeColor.male.prop.matrix
```

**Example** If blue eyes, then p of hair color?

```{r}

###################
### One:

# p(r|c) = p(r,c)/p(r)
# => p(blond hair|blue eyes) = p(blond hair AND blue eyes) / p(blue eyes)

HairEyeColor.male.prop.matrix[2,]

# p(blue eyes)
HairEyeColor.male.prop.matrix$Marginal_Eye[2]

# p(blond hair AND blue eyes)
HairEyeColor.male.prop.matrix[2,4]

## p(Hair_color|blue eyes):
HairEyeColor.male.prop.matrix[2,c(1:4)] / HairEyeColor.male.prop.matrix$Marginal_Eye[2]

```

### Example of joint probability p(r|c)*p(c)

**Disease**
Rare disease with p(theta=disease) = 0.001.

**Test**
Test with sensitivity of 99%. Denote a positive test as T = +, negative test is T = -

`p(T=+|theta=disease) = 0.99` 

Test has a false alarm rate of 5%

`p(T=+|theta=no_disease) = 0.05`

What if a person is drawn and the test is positive?

`p(theta = disease|T=+)`


```{r}

# P(D) = 0.001
# P(nD) = 0.999

#     | +               | -                   |
#     _________________________________________
# D   | 0.99*0.001      | 0.05*(1-0.99)       |
# nD  | (1-0.99)*0.001  | (1-0.05)*(1-0.001)  |
#     _________________________________________


#     | +               | -                   |
#     _________________________________________
# D   | 0.00099         | 5e-04               |
# nD  | 1e-05           | 0.94905             |
#     _________________________________________

0.99*0.001/(0.99*0.001+0.05*(1-0.001))

```

## 5.2 Applied to parameters and data

The key application that makes Bayes' rule so useful is when the row variable represents data values and the column variable represents parameter values. 
A model of data specifies the probability of particular data values given the model’s structure and parameter values. 

The model also indicates the probability of the various parameter values. 

In other words, a model specifies and we use Bayes' rule to convert that to what we really want to know, which is how strongly we should believe in the various parameter values, given the data:

`p(parameters values | data values)` 

![](images/5_7.png)

## 5.3 Complete examples:

> When I refer to the “bias” in a coin, I will sometimes be referring to its underlying probability of coming up heads.

Thus, when a coin is fair, it has a “bias” of 0.50.
Bias is denoted as `theta`.

* We will denote the outcome of a flip as y. 

y = 1 when heads, y = 0 when tails

* The next step in Bayesian data analysis is creating a descriptive model with meaningful parameters.

p(y = 1) of heads: `p(y=1 | theta) = theta`
p(y = 0) of tails: `p(y=0 | theta) = 1-theta`
Combined: `p(y|theta) = theta^y * (1-theta)^(1-y)` (**Bernoulli distribution**)

* The next steps of Bayesian data analysis (recall Section 2.3, p. 25) are collecting the data and applying Bayes' rule to re-allocate credibility across the possible parameter values.

The posterior is a compromise between the prior distribution and the likelihood function.

```{r}

source("Chapters/DBDA2Eprograms/DBDA2E-utilities.R")
source("Chapters/DBDA2Eprograms/BernGrid.R")

Theta = seq(0, 1, length=1001) # Specify fine comb for Theta.
head(Theta)
hist(Theta)

pTheta = pmin(Theta, 1-Theta) # Triangular shape for pTheta.
head(pTheta)
hist(pTheta)

pTheta = pTheta/sum(pTheta) # Make pTheta sum to 1.0
head(pTheta)
hist(pTheta)

Data = c(rep(0,3),rep(1,1)) # Same as c(0,0,0,1). 25% heads with N=4.
head(Data)

posterior = BernGrid(Theta, 
                     pTheta, 
                     Data, 
                     plotType="Bars",
                     showCentTend="Mode",
                     showHDI=TRUE,
                     showpD=FALSE)
# saveGraph(file="BernGridExample",type="jpg")

head(posterior)


```

### Drill down example

```{r}

###########################
### ARGUMENTS
Theta = seq(0, 1, length=1001) 

pTheta = pmin(Theta, 1-Theta) 
pTheta = pTheta/sum(pTheta)

Data = rbinom(n = 7,1,0.8)

### END ARGUMENTS
###########################

  # Create summary values of Data
  z = sum( Data ) # number of 1's in Data
  z
  N = length( Data ) 
  N
  
  # Compute the Bernoulli likelihood at each value of Theta:
  pDataGivenTheta = Theta^z * (1-Theta)^(N-z)
  
  head(Theta)
  head(pDataGivenTheta)
  # Compute the evidence and the posterior via Bayes' rule:
  pData = sum( pDataGivenTheta * pTheta )
  head(pData)
  pThetaGivenData = pDataGivenTheta * pTheta / pData
  head(pThetaGivenData)
  
  # Plot the results.
  layout( matrix( c( 1,2,3 ) ,nrow=3 ,ncol=1 ,byrow=FALSE ) ) # 3x1 panels
  par( mar=c(3,3,1,0) , mgp=c(2,0.7,0) , mai=c(0.5,0.5,0.3,0.1) ) # margins
  cexAxis = 1.33
  cexLab = 1.75
  # convert plotType to notation used by plot:
  plotType="h" 
  dotsize = 5 # how big to make the plotted dots
  barsize = 5 # how wide to make the bar lines    
  # If the comb has a zillion teeth, it's too many to plot, so plot only a
  # thinned out subset of the teeth.
  nteeth = length(Theta)
  thinIdx = 1:nteeth 
  

  # Plot the prior.
  yLim = c(0,1.1*max(c(pTheta,pThetaGivenData)))
  plot( Theta[thinIdx] , pTheta[thinIdx] , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab=bquote(p(theta)) , cex.lab=cexLab ,
        main="Prior" , cex.main=1.5 , col="skyblue" )
  # Plot the likelihood: p(Data|Theta)
  plot( Theta[thinIdx] , pDataGivenTheta[thinIdx] , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=c(0,1.1*max(pDataGivenTheta)) , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab=bquote( "p(D|" * theta * ")" ) , cex.lab=cexLab ,
        main="Likelihood" , cex.main=1.5 , col="skyblue" )
  # Plot the posterior.
  yLim = c(0,1.1*max(c(pTheta,pThetaGivenData)))
  plot( Theta[thinIdx] , pThetaGivenData[thinIdx] , type=plotType , 
        pch="." , cex=dotsize , lwd=barsize ,
        xlim=c(0,1) , ylim=yLim , cex.axis=cexAxis ,
        xlab=bquote(theta) , ylab=bquote( "p(" * theta * "|D)" ) , cex.lab=cexLab ,
        main="Posterior" , cex.main=1.5 , col="skyblue" )
  

```

## Markov chain Monte Carlo (MCMC) methods.

> Involves randomly sampling a large number of representative combinations of parameter values from the posterior distribution.

What makes these methods so useful is that they can generate representative parameter-value combinations from the posterior distribution of complex models without computing the integral in Bayes' rule. 

It is the development of these MCMC methods that has allowed Bayesian statistical methods to gain practical use. This book focuses on MCMC methods for realistic data analysis.

## Exercises

### Exercise 5.1

> [Purpose: Iterative application of Bayes' rule, and seeing how posterior probabilities change with inclusion of more data.] This exercise extends the ideas of Table 5.4, so at this time, please review Table 5.4 and its discussion in the text. Suppose that the same randomly selected person as in Table 5.4 gets re-tested after the first test result was positive, and on the re-test, the result is negative. When taking into account the results of both tests, what is the probability that the person has the disease? Hint: For the prior probability of the re-test, use the posterior computed from the Table 5.4. etain as many decimal places as possible, as rounding can have a surprisingly big effect on the results. One way to avoid unnecessary rounding is to do the calculations in .

```{r}

p_prior = 0.99*0.001/(0.99*0.001+0.05*(1-0.001))
p_prior

(1-0.99)*p_prior/((1-0.99)*p_prior+(1-0.05)*(1-p_prior))
```

### Exercise 5.2

> [Purpose: Getting an intuition for the previous results by using “natural frequency” and “Markov” representations]

> (A) Suppose that the population consists of 100,000 people. Compute how many people would be expected to fall into each cell of Table 5.4. To compute the expected frequency of people in a cell, just multiply the cell probability by the size of the population. To get you started, a few of the cells of the frequency table are filled in here: Notice the frequencies on the lower margin of the table. They indicate that out of 100,000 people, only 100 have the disease, while 99,900 do not have the disease. These marginal frequencies instantiate the prior probability that p(θ =) = 0.001. Notice also the cell frequencies in the column θ =, which indicate that of 100 people with the disease, 99 have a positive test result and 1 has a negative test result. These cell frequencies instantiate the hit rate of 0.99. Your job for this part of the exercise is to fill in the frequencies of the remaining cells of the table. 

```{r}

df.5_4 = 
  data.frame(matrix(c(0.99*0.001,(1-0.99)*0.001,0.05*(1-0.001),(1-0.05)*(1-0.001)),ncol=2,nrow=2))
rownames(df.5_4) = c("T+","T-")
colnames(df.5_4) = c("D","nD")
df.5_4

n = 100000
df.5_4*n
```



> (B) Take a good look at the frequencies in the table you just computed for the previous part. These are the so-called “natural frequencies” of the events, as opposed to the somewhat unintuitive expression in terms of conditional probabilities (Gigerenzer & Hoffrage, 1995). From the cell frequencies alone, determine the proportion of people who have the disease, given that their test result is positive. Before computing the exact answer arithmetically, first give a rough intuitive answer merely by looking at the relative frequencies in the row D = +. Does your intuitive answer match the intuitive answer you provided when originally reading about Table 5.4? Probably not. Your intuitive answer here is probably much closer to the correct answer. Now compute the exact answer arithmetically. It should match the result from applying Bayes' rule in Table 5.4. 

```{r}
99/5094
```

> (C) Now we’ll consider a related representation of the probabilities in terms of natural frequencies, which is especially useful when we accumulate more data. This type of representation is called a “Markov” representation by Krauss, Martignon, and Hoffrage (1999). Suppose now we start with a population of N = 10,000,000 people. We expect 99.9% of them (i.e., 9,990,000) not to have the disease, and just 0.1% (i.e., 10,000) to have the disease. Now consider how many people we expect to test positive. Of the 10,000 people who have the disease, 99%, (i.e., 9,900) will be expected to test positive. Of the 9,990,000 people who do not have the disease, 5% (i.e., 499,500) will be expected to test positive. Now consider re-testing everyone who has tested positive on the first test. How many of them are expected to show a negative result on the retest? Use this diagram to compute your answer: When computing the frequencies for the empty boxes above, be careful to use the proper conditional probabilities! 


```{r}
# 10,000,000 => 10,000 test positive => re test
df.5_4 = 
  data.frame(matrix(c(0.001*0.99,
                      0.001*(1-0.99),
                      0.999*0.05,
                      0.999*(1-0.05)),
                    ncol=2,
                    nrow=2))
rownames(df.5_4) = c("T+","T-")
colnames(df.5_4) = c("D","nD")
df.5_4

n = 10000000
df.5_4.n = df.5_4*n
df.5_4.n
```


```{r}
# Left branch
sum(df.5_4.n[,1]) * 0.99
df.5_4.n[1,1] * (1 - 0.99)

# Right branch
sum(df.5_4.n[,2]) * 0.05
df.5_4.n[1,2] * (1 - 0.05)

```

> (D) Use the diagram in the previous part to answer this: What proportion of people, who test positive at first and then negative on retest, actually have the disease? In other words, of the total number of people at the bottom of the diagram in the previous part (those are the people who tested positive then negative), what proportion of them are in the left branch of the tree? How does the result compare with your answer to Exercise 5.1?

```{r}

99 / (99 + 474525)

# Exercise 1

(1-0.001)*p_prior/((1-0.001)*p_prior+(1-0.05)*(1-p_prior))
```

### Exercise 5.3

> [Purpose: To see a hands-on example of data-order invariance.] Consider again the disease and diagnostic test of the previous two exercises. 
> (A)Suppose that a person selected at random from the population gets the test and it comes back negative. Compute the probability that the person has the disease. 

```{r}

df.5_4

pPositiveDisease = 0.99
pPositiveNoDisease = 0.05
pDisease = 0.001

# Negative
pDiseaseNoNegative = ( (1-pPositiveDisease) * pDisease / ( (1 - pPositiveDisease) * pDisease + (1-pPositiveNoDisease) * (1-pDisease) ) ) 
pDiseaseNoNegative


```


>(B) The person then gets re-tested, and on the second test the result is positive. Compute the probability that the person has the disease. How does the result compare with your answer to Exercise 5.1?

```{r}

# Negative -> Positive
pDiseaseNew = pDiseaseNoNegative
( (pPositiveDisease) * pDiseaseNew / ( (pPositiveDisease) * pDiseaseNew + (pPositiveNoDisease) * (1-pDiseaseNew) ) ) 


```


### Exercise 5.4

> [Purpose: To gain intuition about Bayesian updating by using BernGrid.] Open the program BernGridExample.R. You will notice there are several examples of using the function BernGrid. un the script. For each example, include the  code and the resulting graphic and explain what idea the example illustrates. Hints: Look back at Figures 5.2 and 5.3, and look ahead to Figure 6.5. Two of the examples involve a single flip, with the only difference between the examples being whether the prior is uniform or contains only two extreme options. The point of those two examples is to show that a single datum implies little when the prior is vague, but a single datum can have strong implications when the prior allows only two very different possibilities.

```{r}


Theta = seq( 0 , 1 , length=5 )  # Sparse teeth for Theta.
pTheta = pmin( Theta , 1-Theta ) # Triangular shape for pTheta.
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,0),rep(1,1))      # Single flip with 1 head

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="None" , showHDI=FALSE , showpD=FALSE )

#------------------------------------------------------------------------------

Theta = seq( 0 , 1 , length=11 )  # Sparse teeth for Theta.
pTheta = pmin( Theta , 1-Theta ) # Triangular shape for pTheta.
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,0),rep(1,1))      # Single flip with 1 head

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="None" , showHDI=FALSE , showpD=FALSE )

#------------------------------------------------------------------------------

Theta = seq( 0 , 1 , length=1001 ) # Fine teeth for Theta.
pTheta = rep(1,length(Theta))      # Uniform (horizontal) shape for pTheta.
pTheta = pTheta/sum(pTheta)        # Make pTheta sum to 1.0
Data = c(rep(0,0),rep(1,1))        # Single flip with 1 head

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="None" , showHDI=FALSE , showpD=FALSE )

#------------------------------------------------------------------------------

Theta = seq( 0 , 1 , length=1001 ) # Fine teeth for Theta.
pTheta = rep(0,length(Theta))      # Only extremes are possible!
pTheta[2] = 1                      # Only extremes are possible!
pTheta[length(pTheta)-1] = 1
pTheta = pTheta/sum(pTheta)        # Make pTheta sum to 1.0
Data = c(rep(0,0),rep(1,1))        # Single flip with 1 head

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="None" , showHDI=FALSE , showpD=FALSE )

#------------------------------------------------------------------------------



Theta = seq( 0 , 1 , length=1001 )  # Fine teeth for Theta.
pTheta = pmin( Theta , 1-Theta ) # Triangular shape for pTheta.
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,3),rep(1,1))      # 25% heads, N=4

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="Mode" , showHDI=TRUE , showpD=FALSE )

#------------------------------------------------------------------------------

Theta = seq( 0 , 1 , length=1001 )  # Fine teeth for Theta.
pTheta = pmin( Theta , 1-Theta ) # Triangular shape for pTheta.
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
pTheta = pTheta^10               # Sharpen pTheta !
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,3),rep(1,1))      # 25% heads, N=4

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="Mode" , showHDI=TRUE , showpD=FALSE )

#------------------------------------------------------------------------------

Theta = seq( 0 , 1 , length=1001 )  # Fine teeth for Theta.
pTheta = pmin( Theta , 1-Theta ) # Triangular shape for pTheta.
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
pTheta = pTheta^0.1              # Flatten pTheta !
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,3),rep(1,1))      # 25% heads, N=4

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="Mode" , showHDI=TRUE , showpD=FALSE )

#------------------------------------------------------------------------------


Theta = seq( 0 , 1 , length=1001 )  # Fine teeth for Theta.
pTheta = pmin( Theta , 1-Theta ) # Triangular shape for pTheta.
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,30),rep(1,10))    # 25% heads, N=40

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="Mode" , showHDI=TRUE , showpD=FALSE )

#------------------------------------------------------------------------------

Theta = seq( 0 , 1 , length=1001 )  # Fine teeth for Theta.
pTheta = pmin( Theta , 1-Theta ) # Triangular shape for pTheta.
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
pTheta = pTheta^10               # Sharpen pTheta !
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,30),rep(1,10))    # 25% heads, N=40

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="Mode" , showHDI=TRUE , showpD=FALSE )

#------------------------------------------------------------------------------

Theta = seq( 0 , 1 , length=1001 )  # Fine teeth for Theta.
pTheta = pmin( Theta , 1-Theta ) # Triangular shape for pTheta.
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
pTheta = pTheta^0.1              # Flatten pTheta !
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,30),rep(1,10))    # 25% heads, N=40

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="Mode" , showHDI=TRUE , showpD=FALSE )

#------------------------------------------------------------------------------

Theta = seq( 0 , 1 , length=1000 )  # Fine teeth for Theta.
# Two triangular peaks on a small non-zero floor:
pTheta = c( rep(1,200),seq(1,100,length=50),seq(100,1,length=50),rep(1,200) ,
            rep(1,200),seq(1,100,length=50),seq(100,1,length=50),rep(1,200) )
pTheta = pTheta/sum(pTheta)      # Make pTheta sum to 1.0
Data = c(rep(0,13),rep(1,14))

openGraph(width=5,height=7)
posterior = BernGrid( Theta, pTheta , Data , plotType="Bars" ,
                      showCentTend="None" , showHDI=FALSE , showpD=FALSE )

#------------------------------------------------------------------------------

```



















